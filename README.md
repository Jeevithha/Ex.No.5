# EXP 5: COMPARATIVE ANALYSIS OF DIFFERENT TYPES OF PROMPTING PATTERNS AND EXPLAIN WITH VARIOUS TEST SCENARIOS
# NAME : JEEVITHA S
# REG.NO : 212222100016
# Aim: To test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios.  Analyze the quality, accuracy, and depth of the generated responses 

## Basic Prompting (Direct Instruction): 
This is the simplest form, where you directly instruct the Al to perform a task. 
## Example: 
"Write a short poem about a cat." 
### Test Scenario: 
Evaluate the Al's ability to follow simple instructions and generate relevant output. Vary the complexity of the request.
## Question Answering:
Framing the prompt as a question to elicit a specific answer. 
### Example:
"What is the capital of France?" 
## Test Scenario: 
Assess the Al's knowledge base and its ability to extract and present factual information accurately.
## Few-Shot Prompting (N-Shot Prompting):
Providing a few examples of the desired input-output
format to guide the Al.
## Example:
This experimental model uses your Search history. Some features aren't available. Here is the
comparative analysis of different prompting patterns and their related definitions without
Comparative Analysis of Different Types of Prompting Patterns and Explain with Various Test
Scenarios
## Basic Prompting (Direct Instruction): 
This is the simplest form, where you directly instruct the Al to
### perform a task. Example: 
"Write a short poem about a cat."
## Test Scenario: 
Evaluate the Al's ability to follow simple instructions and generate relevant output. Vary the complexity of the request.
## Question Answering: Framingthe prompt as a question to elicit a specific answer. 
### Example: 
"What is the capital of France?"
### Test Scenario:
Assess the Al's knowledge base and its ability to extract and present factual
information accurately.
### Few-Shot Prompting (N-Shot Prompting): 
Providing a few examples of the desired input-output format to guide the Al.
### Example:
Input: The sky is blue.
Output: The sky is blue because of the way sunlight scatters off the atmosphere.
Input: The grass is green.
Output: The grass is green because of the presence of chlorophyll in its cells.
Input: The sun is hot.
Output:
## Test Scenario:
Check if the Al can learn from the provided examples and apply the pattern to new, similar inputs. Vary the number and relevance of the examples.

## Creativity Test: 
Assess the originality and imaginativeness of the Al's generated content (if applicable). This measures the novelty, originality, and imagination of the model's generated
content based on different prompting styles. We might use basic prompts, persona prompts, or open-ended prompts and evaluate the diversity and uniqueness of the generated stories,
poems, or ideas.
## Bias Detection Test:
Examine the Al's responses for any potential biases related to gender, race, or other sensitive attributes. This aims to identify if a prompting pattern inadvertently
triggers or amplifies biases (e.g., gender, racial) in the model's responses. We would use
prompts designed to be neutral but potentially sensitive to bias and observe the model's
output across different prompting techniques.
## Robustness Test: 
Test the Al's performance with variations in the prompt, including typos, grammatical errors, or ambiguous phrasing. This evaluates the consistency and reliability of a prompting pattern across slight variations in the input prompt or when faced with noisy or slightly ambiguous queries. We would test different patterns by introducing minor changes
to the prompts and observe how the model's output stability. 
## Scalability Test: 
See how the Al handles a large volume of prompts or increasingly complex requests. This examines how well a prompting pattern performs when applied to a larger number of diverse tasks or when the complexity of the tasks increases. We would compare the effectiveness of different patterns across a wide range of prompts and task types. This analysis provides a foundational understanding of various prompting patterns and how they can
be tested.
## Conclusion:
Creativity tests favor more open or role-playing prompts, whereas bias detection aims to identify
patterns that trigger unfair outputs. Robustness is evaluated by how consistently a pattern performs
with slight prompt variations. Scalability looks at the pattern's efficiency and effectiveness across
many tasks. The choice of prompting technique directly impacts the outcomes of these tests,
highlighting the importance of selecting the appropriate pattern based on the specific evaluation
goal. Ultimately, understanding these differences allows for optimized prompt engineering to achieve
desired model behaviors in various contexts.
